# Transformer configuration for single GPU training.

model_dir: trained_models

data:
  train_features_file: ../../Data/twitter/opennmt_twitter/opennmt_twitter_source.txt
  train_labels_file: ../../Data/twitter/opennmt_twitter/opennmt_twitter_target.txt

  source_words_vocabulary: ../../Data/twitter/opennmt_twitter/opennmt_twitter_source_vocab.txt
  target_words_vocabulary: ../../Data/twitter/opennmt_twitter/opennmt_twitter_target_vocab.txt

params:
  optimizer: LazyAdamOptimizer
  learning_rate: 2.0  # The scale constant.
  decay_type: noam_decay
  decay_rate: 512  # Model dimension.
  decay_steps: 4000  # Warmup steps.

  # Divide this value by the total number of GPUs used.
  decay_step_duration: 8  # 1 decay step is 8 training steps.

  average_loss_in_time: true
  label_smoothing: 0.1

  beam_width: 4
  length_penalty: 0.6

train:
  batch_size: 3072
  batch_type: tokens
  bucket_width: 1

  maximum_features_length: 70
  maximum_labels_length: 70

  save_checkpoints_steps: 5000
  keep_checkpoint_max: 8
  save_summary_steps: 100
  train_steps: 1000000

  # Consider setting this to -1 to match the number of training examples.
  sample_buffer_size: 100000

eval:
  batch_size: 64
  eval_delay: 18000  # Every 5 hours.

infer:
  batch_size: 32